{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956cde40-5eaa-4466-a6be-0a4f10b22222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import v2\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import time\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2d5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://pytorch-tutorials-preview.netlify.app/beginner/transformer_tutorial.html\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 dropout: float = 0.1, \n",
    "                 max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ae2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 d_query: int = 128, \n",
    "                 n_heads: int = 8,\n",
    "                 device: torch.device = torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_query)\n",
    "        self.W_k = nn.Linear(d_model, d_query)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.scaling_factor = math.sqrt(d_query)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        \n",
    "        x = x.unsqueeze(1).repeat([1, self.n_heads, 1, 1])\n",
    "        \n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        attention_pattern = torch.matmul(q, torch.transpose(k, -2, -1)) / self.scaling_factor\n",
    "        \n",
    "        seq_len = attention_pattern.shape[-1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(self.device)\n",
    "        attention_pattern = torch.masked_fill(attention_pattern, mask, float(\"-inf\"))\n",
    "\n",
    "        attention_pattern = self.softmax(attention_pattern)\n",
    "\n",
    "        output = torch.sum(torch.matmul(attention_pattern, v), dim=1)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd25bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 d_up: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Linear(d_model, d_up)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down = nn.Linear(d_up, d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        output = self.up(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.down(output)\n",
    "\n",
    "        output = output + x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cbf8aa-4165-4b6f-963d-f1bbb138d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_vocab: int, \n",
    "                 d_model: int = 128, \n",
    "                 d_query: int = 128, \n",
    "                 n_heads: int = 8, \n",
    "                 n_layers: int = 4, \n",
    "                 d_up: int = 256,\n",
    "                 device: torch.device = torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, max_len=50000)\n",
    "\n",
    "        self.attention_layers = nn.ModuleList([layer for _ in range(n_layers) for layer in \n",
    "                                               (SelfAttention(d_model, d_query, n_heads, device), \n",
    "                                                nn.LayerNorm(d_model),\n",
    "                                                MultilayerPerceptron(d_model, d_up))])\n",
    "\n",
    "        # self.self_attention = SelfAttention(d_model, d_query, n_heads, device)\n",
    "        # self.mlp = MultilayerPerceptron(d_model, d_up)\n",
    "\n",
    "        self.unembedding = nn.Linear(d_model, n_vocab)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for layer in self.attention_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.unembedding(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa09e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint():\n",
    "    def __init__(self, model_state, optim_state, epoch: int, batch: int, rng_state: torch.Tensor):\n",
    "        self.model_state = model_state\n",
    "        self.optim_state = optim_state\n",
    "        self.epoch       = epoch\n",
    "        self.batch       = batch\n",
    "        self.rng_state   = rng_state\n",
    "    def save(self, file_path):\n",
    "        torch.save({\n",
    "            \"model_state\": self.model_state,\n",
    "            \"optim_state\": self.optim_state,\n",
    "            \"epoch\"      : self.epoch,\n",
    "            \"batch\"      : self.batch,\n",
    "            \"rng_state\"  : self.rng_state\n",
    "        }, file_path)\n",
    "        print(f\"Model checkpoint saved at {file_path} at epoch {self.epoch} batch {self.batch}\")\n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        return ModelCheckpoint(checkpoint[\"model_state\"], \n",
    "                               checkpoint[\"optim_state\"], \n",
    "                               checkpoint[\"epoch\"], \n",
    "                               checkpoint[\"batch\"], \n",
    "                               checkpoint[\"rng_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointRandomSampler(torch.utils.data.RandomSampler):\n",
    "    def __init__(self, data_source, checkpoint: ModelCheckpoint = None):\n",
    "        super().__init__(data_source)\n",
    "        self.start_idx = 0 if checkpoint == None else checkpoint.batch\n",
    "    def __iter__(self):\n",
    "        batch_idxs = list(super().__iter__())\n",
    "        for idx in batch_idxs[self.start_idx:]:\n",
    "            yield idx\n",
    "        return\n",
    "    def __len__(self):\n",
    "        return super().__len__() - self.start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3c36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, \n",
    "                   device, \n",
    "                   criterion, \n",
    "                   test_loader):\n",
    "    with torch.no_grad():\n",
    "        avg_loss = 0\n",
    "        for idx, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = inputs[:,1:]\n",
    "            outputs = model(inputs)[:,:-1,:]\n",
    "\n",
    "            targets = targets.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            avg_loss += loss\n",
    "\n",
    "        avg_loss /= len(test_loader)\n",
    "        print(f\"VALIDATE: Average Loss: {avg_loss}\")\n",
    "\n",
    "def train_model(model: nn.Module, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                device, \n",
    "                train_loader, \n",
    "                validation_loader, \n",
    "                accum_steps,\n",
    "                num_epochs: int = 4,\n",
    "                checkpoint: ModelCheckpoint = None):\n",
    "    \n",
    "    epoch = 0\n",
    "    start_batch = 0\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(checkpoint.model_state)\n",
    "        optimizer.load_state_dict(checkpoint.optim_state)\n",
    "        epoch = checkpoint.epoch\n",
    "        start_batch = checkpoint.batch\n",
    "        torch.set_rng_state(checkpoint.rng_state)\n",
    "\n",
    "    print(f\"Starting training on epoch {epoch}, batch {start_batch}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        for idx, inputs in enumerate(train_loader):\n",
    "            batch_idx = start_batch + idx\n",
    "            inputs = inputs.to(device)\n",
    "            targets = inputs[:,1:]\n",
    "            outputs = model(inputs)[:,:-1,:]\n",
    "\n",
    "            targets = targets.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "            \n",
    "            loss = criterion(outputs, targets) / accum_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx + 1) % (accum_steps * 4) == 0:\n",
    "                print(f\"Epoch [{epoch}].[{batch_idx}] Loss: {loss * accum_steps}\")\n",
    "\n",
    "            if (batch_idx + 1) % (accum_steps * 16) == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"TIME: {elapsed_time / (accum_steps * 16)} seconds per batch\")\n",
    "                start_time = time.time()\n",
    "\n",
    "                allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "                print(f\"USAGE: Allocated {allocated:.2f}GB, Reserved {reserved:.2f}GB, Peak: {peak:.2f}GB\")\n",
    "\n",
    "            if (batch_idx + 1) % (accum_steps * 32) == 0:\n",
    "                validate_model(model, device, criterion, validation_loader)\n",
    "                checkpoint = ModelCheckpoint(model.state_dict(), \n",
    "                                            optimizer.state_dict(), \n",
    "                                            epoch, \n",
    "                                            batch_idx, \n",
    "                                            torch.get_rng_state())\n",
    "                checkpoint.save(\"checkpoint.pt\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81cedc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86c0e410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2141709\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train+validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7d5ead-bd78-46a6-82cd-cdcec26fe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38761c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequence, encoder):\n",
    "    # NOTE: <EOS> token is encoder.n_vocab\n",
    "    sequence[\"text\"] = torch.tensor(encoder.encode(sequence[\"text\"]) + [encoder.n_vocab], dtype=torch.int64)\n",
    "    return sequence\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, num_proc=8, fn_kwargs={\"encoder\": encoder}).with_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c17f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset[\"train\"][\"text\"]\n",
    "\n",
    "test_set = tokenized_dataset[\"test\"].train_test_split(test_size=0.001, shuffle=True)\n",
    "test = test_set[\"train\"][\"text\"]\n",
    "validation = test_set[\"test\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 2\n",
    "accum_steps = 32\n",
    "d_model  = 256\n",
    "d_query  = 64\n",
    "d_up = 512\n",
    "n_heads  = 4\n",
    "n_layers = 4\n",
    "\n",
    "# NOTE: add 1 for <EOS> token\n",
    "n_vocab  = encoder.n_vocab + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7887f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100277\n"
     ]
    }
   ],
   "source": [
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29143c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padding(batch):\n",
    "    batch = pad_sequence(batch, batch_first=True)\n",
    "    return batch\n",
    "\n",
    "checkpoint_path = \"checkpoint.pt\"\n",
    "\n",
    "checkpoint = None\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = ModelCheckpoint.load(checkpoint_path)\n",
    "\n",
    "\n",
    "sampler = CheckpointRandomSampler(train, checkpoint)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn_padding)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padding)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padding)\n",
    "\n",
    "model = Transformer(n_vocab=n_vocab, \n",
    "                    d_model=d_model, \n",
    "                    d_query=d_query, \n",
    "                    n_heads=n_heads, \n",
    "                    n_layers=n_layers, \n",
    "                    d_up=d_up, \n",
    "                    device=device).to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9788985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on epoch 0, batch 236543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 22:51:36.530000 139836 torch/_inductor/utils.py:1613] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0].[236543] Loss: 2.5811259746551514\n",
      "TIME: 0.013678241521120071 seconds per batch\n",
      "USAGE: Allocated 1.15GB, Reserved 1.95GB, Peak: 1.72GB\n",
      "VALIDATE: Average Loss: 2.99733567237854\n",
      "Model checkpoint saved at checkpoint.pt at epoch 0 batch 236543\n",
      "Epoch [0].[236671] Loss: 2.6559653282165527\n",
      "Epoch [0].[236799] Loss: 3.5133161544799805\n",
      "Epoch [0].[236927] Loss: 3.46524715423584\n",
      "Epoch [0].[237055] Loss: 2.581388235092163\n",
      "TIME: 0.03390326304361224 seconds per batch\n",
      "USAGE: Allocated 1.12GB, Reserved 5.08GB, Peak: 4.63GB\n",
      "Epoch [0].[237183] Loss: 3.7092299461364746\n",
      "Epoch [0].[237311] Loss: 3.644249200820923\n",
      "Epoch [0].[237439] Loss: 3.072859525680542\n",
      "Epoch [0].[237567] Loss: 3.0596001148223877\n",
      "TIME: 0.02334205713123083 seconds per batch\n",
      "USAGE: Allocated 1.17GB, Reserved 5.88GB, Peak: 4.63GB\n",
      "VALIDATE: Average Loss: 2.993046998977661\n",
      "Model checkpoint saved at checkpoint.pt at epoch 0 batch 237567\n",
      "Epoch [0].[237695] Loss: 3.2633605003356934\n",
      "Epoch [0].[237823] Loss: 3.0824472904205322\n",
      "Epoch [0].[237951] Loss: 2.6170573234558105\n",
      "Epoch [0].[238079] Loss: 3.1394548416137695\n",
      "TIME: 0.029597706627100706 seconds per batch\n",
      "USAGE: Allocated 1.27GB, Reserved 5.88GB, Peak: 4.63GB\n",
      "Epoch [0].[238207] Loss: 2.314873218536377\n",
      "Epoch [0].[238335] Loss: 2.7572085857391357\n",
      "Epoch [0].[238463] Loss: 3.4671244621276855\n",
      "Epoch [0].[238591] Loss: 2.5194146633148193\n",
      "TIME: 0.023163202218711376 seconds per batch\n",
      "USAGE: Allocated 1.31GB, Reserved 5.88GB, Peak: 4.63GB\n",
      "VALIDATE: Average Loss: 2.9817545413970947\n",
      "Model checkpoint saved at checkpoint.pt at epoch 0 batch 238591\n",
      "Epoch [0].[238719] Loss: 2.2094709873199463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, criterion, device, train_loader, validation_loader, accum_steps, num_epochs, checkpoint)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m     49\u001b[39m     batch_idx = start_batch + idx\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     inputs = \u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     targets = inputs[:,\u001b[32m1\u001b[39m:]\n\u001b[32m     52\u001b[39m     outputs = model(inputs)[:,:-\u001b[32m1\u001b[39m,:]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(model=model, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            train_loader=train_loader, \n",
    "            validation_loader=validation_loader, \n",
    "            accum_steps=accum_steps,\n",
    "            checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "482a1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, encoder: tiktoken.Encoding, device: torch.device, prompt: str):\n",
    "    sequence = encoder.encode(prompt)\n",
    "    with torch.no_grad():\n",
    "        num_sentences = 0\n",
    "        while num_sentences < 15:\n",
    "            input = torch.tensor(sequence, dtype=torch.int64).unsqueeze(0).to(device)\n",
    "            output = model(input)\n",
    "            output = output[0,-1,:].argmax().item()\n",
    "            sequence = sequence + [output]\n",
    "\n",
    "            if output == encoder.encode(\".\"):\n",
    "                num_sentences += 1\n",
    "\n",
    "        response = encoder.decode(sequence)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0324ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e6150ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# checkpoint = ModelCheckpoint.load(\"checkpoint.pt\")\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# model.load_state_dict(checkpoint.model_state)\u001b[39;00m\n\u001b[32m      4\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mTell me a story about a hungry man\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(model, encoder, device, prompt)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m num_sentences < \u001b[32m15\u001b[39m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28minput\u001b[39m = torch.tensor(sequence, dtype=torch.int64).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     output = output[\u001b[32m0\u001b[39m,-\u001b[32m1\u001b[39m,:].argmax().item()\n\u001b[32m      9\u001b[39m     sequence = sequence + [output]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:414\u001b[39m, in \u001b[36mOptimizedModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001b[32m    405\u001b[39m     warnings.warn(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    413\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:832\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    829\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# self.self_attention = SelfAttention(d_model, d_query, n_heads, device)\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# self.mlp = MultilayerPerceptron(d_model, d_up)\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m.unembedding = nn.Linear(d_model, n_vocab)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     26\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m     27\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pe(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1046\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:1130\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1128\u001b[39m full_args.extend(params_buffers_flat)\n\u001b[32m   1129\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:353\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    351\u001b[39m         torch._C._set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    352\u001b[39m     record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:129\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    131\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    132\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    133\u001b[39m         warnings.warn(\n\u001b[32m    134\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    135\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:526\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    519\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    520\u001b[39m         runtime_metadata,\n\u001b[32m    521\u001b[39m         out,\n\u001b[32m    522\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    523\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    524\u001b[39m     )\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_inductor/output_code.py:613\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    610\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m record_function(\n\u001b[32m    611\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## Call CompiledFxGraph \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._fx_graph_cache_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ##\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    615\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/notebooks/.venv/lib/python3.13/site-packages/torch/_inductor/utils.py:3017\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   3013\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(new_inputs: \u001b[38;5;28mlist\u001b[39m[InputType]) -> Any:\n\u001b[32m   3014\u001b[39m     old_tensors, new_tensors = copy_misaligned_inputs(\n\u001b[32m   3015\u001b[39m         new_inputs, inputs_to_check, mutated_input_idxs\n\u001b[32m   3016\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3017\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3019\u001b[39m     \u001b[38;5;66;03m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001b[39;00m\n\u001b[32m   3020\u001b[39m     \u001b[38;5;66;03m# original tensor.\u001b[39;00m\n\u001b[32m   3021\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(old_tensors):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torchinductor_aidans39/65/c65orkji5uxu7qvm2kh5rvisgkncit7h6n2oobmcjv3ieqqql3ce.py:806\u001b[39m, in \u001b[36mRunner.call\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m arg51_1\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buf62\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m buf67 = \u001b[43mempty_strided_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms27\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100277\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100277\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [output_18, output_19, x_11], Original ATen: [aten.addmm, aten.view, aten.add, aten.t]\u001b[39;00m\n\u001b[32m    808\u001b[39m extern_kernels.addmm(arg53_1, reinterpret_tensor(buf66, (s27, \u001b[32m256\u001b[39m), (\u001b[32m256\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m), reinterpret_tensor(arg52_1, (\u001b[32m256\u001b[39m, \u001b[32m100277\u001b[39m), (\u001b[32m1\u001b[39m, \u001b[32m256\u001b[39m), \u001b[32m0\u001b[39m), alpha=\u001b[32m1\u001b[39m, beta=\u001b[32m1\u001b[39m, out=buf67)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# checkpoint = ModelCheckpoint.load(\"checkpoint.pt\")\n",
    "# model.load_state_dict(checkpoint.model_state)\n",
    "\n",
    "prompt = \"Tell me a story about a hungry man\"\n",
    "\n",
    "response = generate_response(model, encoder=encoder, device=device, prompt=prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d82d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(encoder.encode(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
