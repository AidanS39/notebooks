{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956cde40-5eaa-4466-a6be-0a4f10b22222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import v2\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import time\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2d5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://pytorch-tutorials-preview.netlify.app/beginner/transformer_tutorial.html\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 dropout: float = 0.1, \n",
    "                 max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ae2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 d_query: int = 128, \n",
    "                 n_heads: int = 8,\n",
    "                 device: torch.device = torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_query)\n",
    "        self.W_k = nn.Linear(d_model, d_query)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.scaling_factor = math.sqrt(d_query)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        \n",
    "        x = x.unsqueeze(1).repeat([1, self.n_heads, 1, 1])\n",
    "        \n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        attention_pattern = torch.matmul(q, torch.transpose(k, -2, -1)) / self.scaling_factor\n",
    "        \n",
    "        seq_len = attention_pattern.shape[-1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(self.device)\n",
    "        attention_pattern = torch.masked_fill(attention_pattern, mask, float(\"-inf\"))\n",
    "\n",
    "        attention_pattern = self.softmax(attention_pattern)\n",
    "\n",
    "        output = torch.sum(torch.matmul(attention_pattern, v), dim=1)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd25bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 d_up: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Linear(d_model, d_up)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down = nn.Linear(d_up, d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        output = self.up(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.down(output)\n",
    "\n",
    "        output = output + x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cbf8aa-4165-4b6f-963d-f1bbb138d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_vocab: int, \n",
    "                 d_model: int = 128, \n",
    "                 d_query: int = 128, \n",
    "                 n_heads: int = 8, \n",
    "                 n_layers: int = 4, \n",
    "                 d_up: int = 256,\n",
    "                 device: torch.device = torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, max_len=50000)\n",
    "\n",
    "        self.attention_layers = nn.ModuleList([layer for _ in range(n_layers) for layer in \n",
    "                                               (SelfAttention(d_model, d_query, n_heads, device), \n",
    "                                                nn.LayerNorm(d_model),\n",
    "                                                MultilayerPerceptron(d_model, d_up))])\n",
    "\n",
    "        # self.self_attention = SelfAttention(d_model, d_query, n_heads, device)\n",
    "        # self.mlp = MultilayerPerceptron(d_model, d_up)\n",
    "\n",
    "        self.unembedding = nn.Linear(d_model, n_vocab)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for layer in self.attention_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.unembedding(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa09e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint():\n",
    "    def __init__(self, model_state, optim_state, epoch: int, batch: int, rng_state: torch.Tensor):\n",
    "        self.model_state = model_state\n",
    "        self.optim_state = optim_state\n",
    "        self.epoch       = epoch\n",
    "        self.batch       = batch\n",
    "        self.rng_state   = rng_state\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        print(\"Saving model checkpoint.... DO NOT EXIT\")\n",
    "        torch.save({\n",
    "            \"model_state\": self.model_state,\n",
    "            \"optim_state\": self.optim_state,\n",
    "            \"epoch\"      : self.epoch,\n",
    "            \"batch\"      : self.batch,\n",
    "            \"rng_state\"  : self.rng_state\n",
    "        }, file_path)\n",
    "        print(f\"Model checkpoint saved at {file_path} at epoch {self.epoch} batch {self.batch}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        return ModelCheckpoint(checkpoint[\"model_state\"], \n",
    "                               checkpoint[\"optim_state\"], \n",
    "                               checkpoint[\"epoch\"], \n",
    "                               checkpoint[\"batch\"], \n",
    "                               checkpoint[\"rng_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointRandomSampler(torch.utils.data.RandomSampler):\n",
    "    def __init__(self, data_source, batch_size, checkpoint: ModelCheckpoint = None):\n",
    "        super().__init__(data_source)\n",
    "        self.start_batch_idx = 0 if checkpoint == None else checkpoint.batch\n",
    "        self.batch_size = batch_size\n",
    "    def __iter__(self):\n",
    "        idxs = list(super().__iter__())\n",
    "        for idx in idxs[(self.start_batch_idx*self.batch_size):]:\n",
    "            yield idx\n",
    "        return\n",
    "    def __len__(self):\n",
    "        return super().__len__() - self.start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8546f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage() -> None:\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"USAGE: Allocated {allocated:.2f}GB, Reserved {reserved:.2f}GB, Peak: {peak:.2f}GB\")\n",
    "\n",
    "def print_avg_batch_time(start_time, num_batches) -> None:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"TIME: {elapsed_time / (num_batches)} seconds per batch\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, batch_idx, checkpoint_path):\n",
    "    checkpoint = ModelCheckpoint(model.state_dict(), \n",
    "                                optimizer.state_dict(), \n",
    "                                epoch, \n",
    "                                batch_idx, \n",
    "                                torch.get_rng_state())\n",
    "    checkpoint.save(checkpoint_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3c36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, \n",
    "                   device, \n",
    "                   criterion, \n",
    "                   test_loader):\n",
    "    with torch.no_grad():\n",
    "        avg_loss = 0\n",
    "        for idx, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = inputs[:,1:]\n",
    "            outputs = model(inputs)[:,:-1,:]\n",
    "\n",
    "            targets = targets.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            avg_loss += loss\n",
    "\n",
    "        avg_loss /= len(test_loader)\n",
    "        print(f\"VALIDATE: Average Loss: {avg_loss}\")\n",
    "\n",
    "def train_model(model: nn.Module, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                device, \n",
    "                train_loader, \n",
    "                validation_loader, \n",
    "                accum_steps,\n",
    "                num_epochs: int = 4,\n",
    "                checkpoint: ModelCheckpoint = None):\n",
    "    \n",
    "    start_epoch = 0\n",
    "    start_batch = 0\n",
    "\n",
    "    checkpoint_path = \"checkpoint.pt\"\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(checkpoint.model_state)\n",
    "        optimizer.load_state_dict(checkpoint.optim_state)\n",
    "        start_epoch = checkpoint.epoch\n",
    "        start_batch = checkpoint.batch\n",
    "        torch.set_rng_state(checkpoint.rng_state)\n",
    "\n",
    "    print(f\"Starting training on epoch {start_epoch}, batch {start_batch}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_idx = start_batch\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        for idx, inputs in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = inputs[:,1:]\n",
    "            outputs = model(inputs)[:,:-1,:]\n",
    "\n",
    "            targets = targets.reshape(-1)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "            \n",
    "            loss = criterion(outputs, targets) / accum_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx + 1) % (accum_steps * 4) == 0:\n",
    "                print(f\"Epoch [{epoch}].[{batch_idx}] Loss: {loss * accum_steps}\")\n",
    "\n",
    "            if (batch_idx + 1) % (accum_steps * 64) == 0:\n",
    "                print_avg_batch_time(start_time, accum_steps * 64)\n",
    "                start_time = time.time()\n",
    "\n",
    "                print_memory_usage()\n",
    "\n",
    "                validate_model(model, device, criterion, validation_loader)\n",
    "\n",
    "                save_checkpoint(model, optimizer, epoch, batch_idx, checkpoint_path)\n",
    "            batch_idx += 1\n",
    "\n",
    "        batch_idx = 0\n",
    "        save_checkpoint(model, optimizer, epoch, batch_idx, checkpoint_path)\n",
    "    epoch = 0\n",
    "    save_checkpoint(model, optimizer, epoch, batch_idx, checkpoint_path)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81cedc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c0e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2141709\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train+validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7d5ead-bd78-46a6-82cd-cdcec26fe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38761c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequence, encoder):\n",
    "    # NOTE: <EOS> token is encoder.n_vocab\n",
    "    sequence[\"text\"] = torch.tensor(encoder.encode(sequence[\"text\"]) + [encoder.n_vocab], dtype=torch.int64)\n",
    "    return sequence\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, num_proc=8, fn_kwargs={\"encoder\": encoder}).with_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c17f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset[\"train\"][\"text\"]\n",
    "\n",
    "test_set = tokenized_dataset[\"test\"].train_test_split(test_size=0.001, shuffle=True)\n",
    "test = test_set[\"train\"][\"text\"]\n",
    "validation = test_set[\"test\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size  = 16\n",
    "accum_steps = 2\n",
    "d_model     = 256\n",
    "d_query     = 64\n",
    "d_up        = 512\n",
    "n_heads     = 4\n",
    "n_layers    = 4\n",
    "num_epochs  = 3\n",
    "\n",
    "# NOTE: add 1 for <EOS> token\n",
    "n_vocab  = encoder.n_vocab + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7887f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100278\n"
     ]
    }
   ],
   "source": [
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29143c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padding(batch):\n",
    "    batch = pad_sequence(batch, batch_first=True)\n",
    "    return batch\n",
    "\n",
    "checkpoint_path = \"checkpoint.pt\"\n",
    "\n",
    "checkpoint = None\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = ModelCheckpoint.load(checkpoint_path)\n",
    "\n",
    "\n",
    "sampler = CheckpointRandomSampler(train, batch_size, checkpoint)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn_padding)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padding)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padding)\n",
    "\n",
    "model = Transformer(n_vocab=n_vocab, \n",
    "                    d_model=d_model, \n",
    "                    d_query=d_query, \n",
    "                    n_heads=n_heads, \n",
    "                    n_layers=n_layers, \n",
    "                    d_up=d_up, \n",
    "                    device=device).to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on epoch 3, batch 0\n",
      "Epoch [3].[7] Loss: 0.5985713601112366\n",
      "Epoch [3].[15] Loss: 1.101233720779419\n",
      "Epoch [3].[23] Loss: 1.210019588470459\n",
      "Epoch [3].[31] Loss: 0.9138473868370056\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 68\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, device, train_loader, validation_loader, accum_steps, num_epochs, checkpoint)\u001b[0m\n\u001b[1;32m     65\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (accum_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m].[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39maccum_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (accum_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     71\u001b[0m     print_avg_batch_time(start_time, accum_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:1108\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;66;03m# Use detach() here to avoid the warning when converting a scalar Tensor that\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;66;03m# requires gradients to a python number. It is ok for formatting.\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model=model, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            device=device, \n",
    "            train_loader=train_loader, \n",
    "            validation_loader=validation_loader, \n",
    "            accum_steps=accum_steps,\n",
    "            num_epochs=3,\n",
    "            checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, encoder: tiktoken.Encoding, device: torch.device, prompt: str):\n",
    "    sequence = encoder.encode(prompt)\n",
    "    with torch.no_grad():\n",
    "        num_chars = 0\n",
    "        end_sequence = False\n",
    "        while end_sequence == False:\n",
    "            input = torch.tensor(sequence, dtype=torch.int64).unsqueeze(0).to(device)\n",
    "            output = model(input)\n",
    "            output = output[0,-1,:].argmax().item()\n",
    "\n",
    "            num_chars += 1\n",
    "            if num_chars >= 1000:\n",
    "                end_sequence = True\n",
    "                \n",
    "            if output == n_vocab - 1:\n",
    "                end_sequence = True\n",
    "            else:\n",
    "                sequence = sequence + [output]\n",
    "                \n",
    "        response = encoder.decode(sequence)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0324ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6150ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a story about a hungry man on a sailboat. He likes to find food and water. He likes to drink water and drink water. He likes to drink water from the water and drink the water. He likes to drink the water and the water.\n",
      "\n",
      "One day, he sees a big fish in the water. He wants to catch the fish. He runs to the fish and tries to catch the fish. But the fish is too fast. He cannot catch the fish.\n",
      "\n",
      "He jumps on the fish and tries to catch the fish. He tries to catch the fish. But the fish is too fast. He cannot catch the fish. He is sad. He cries and cries.\n",
      "\n",
      "Mom hears the fish and comes to the rescue. She sees the fish and the fish. She is not happy. She says to the fisherman. She says, \"I'm sorry, fish. I will help you. I will help you. I will help you.\"\n",
      "\n",
      "The fishermanermanerman takes the fish to the sink. He says, \"I will help you. I will help you. I will help you. I will help you. I will be my friend.\"\n",
      "\n",
      "The fishermanerman takes the fish anderman gives the fish to the fish. Heerman says, \"Thank you, fish. You are a good friend. I will help you. I will help you. I will be my friend.\"\n",
      "\n",
      "The fishermanerman takes the fish andermanerman gives the fish a fish. He says, \"Thank you, fish. You are a good friend. I will be my friend.\"\n",
      "\n",
      "The fishermanerman takes the fish to the fishermanerman and the fishermanerman's boaterman's boat. He says, \"You are a good friend. I will be my friend.\"\n",
      "\n",
      "The fishermanermanerman takes the fisherman's advice. He says, \"Thank you, fish. You are a good friend. I will be my friend. I will be my friend. I will be my friend.\"\n",
      "\n",
      "The fishermanermanermanerman's advice. He says, \"You are a good friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend.\"\n",
      "\n",
      "The fisherman says, \"You are a good friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend too friend too friend. I will be my friend too.\"\n",
      "\n",
      "The fishermanerman. He says, \"I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend too.\"\n",
      "\n",
      "The fisherman. He says, \"Thank you, I will be my friend. I will be my friend. I will be my friend too friend too.\"\n",
      "\n",
      "The fisherman. He is happy. He says, \"You are my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend too.\"\n",
      "\n",
      "The fisherman. He says, \"You are my friend. I am my friend. I will be my friend too friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I will be my friend. I love you. I love you be my friend. I will be my friend. I love you love you.\"\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint.load(\"checkpoint.pt\")\n",
    "model.load_state_dict(checkpoint.model_state)\n",
    "\n",
    "prompt = \"Tell me a story about a hungry man on a sailboat.\"\n",
    "\n",
    "response = generate_response(model, encoder=encoder, device=device, prompt=prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
