{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956cde40-5eaa-4466-a6be-0a4f10b22222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import v2\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2d5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://pytorch-tutorials-preview.netlify.app/beginner/transformer_tutorial.html\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 dropout: float = 0.1, \n",
    "                 max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ae2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 d_query: int = 128, \n",
    "                 n_heads: int = 8,\n",
    "                 device: torch.device = torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_query)\n",
    "        self.W_k = nn.Linear(d_model, d_query)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.scaling_factor = 1 / math.sqrt(d_query)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        attention_pattern = torch.matmul(q, torch.transpose(k, 1, 2)) * self.scaling_factor\n",
    "        \n",
    "        seq_len = attention_pattern.shape[-1]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(self.device)\n",
    "        attention_pattern = torch.masked_fill(attention_pattern, mask, float(\"-inf\"))\n",
    "\n",
    "        attention_pattern = self.softmax(attention_pattern)\n",
    "\n",
    "        output = torch.matmul(attention_pattern, v)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd25bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 d_up: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Linear(d_model, d_up)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down = nn.Linear(d_up, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.up(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.down(output)\n",
    "\n",
    "        output = output + x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cbf8aa-4165-4b6f-963d-f1bbb138d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_vocab: int, \n",
    "                 d_model: int = 128, \n",
    "                 d_query: int = 128, \n",
    "                 n_heads: int = 8, \n",
    "                 n_layers: int = 4, \n",
    "                 d_up: int = 256,\n",
    "                 device: torch.device = torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, max_len=50000)\n",
    "\n",
    "        self.attention_layers = nn.ModuleList([layer for _ in range(n_layers) for layer in \n",
    "                                               (SelfAttention(d_model, d_query, n_heads, device), \n",
    "                                               MultilayerPerceptron(d_model, d_up))])\n",
    "\n",
    "        # self.self_attention = SelfAttention(d_model, d_query, n_heads, device)\n",
    "        # self.mlp = MultilayerPerceptron(d_model, d_up)\n",
    "\n",
    "        self.unembedding = nn.Linear(d_model, n_vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for layer in self.attention_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.unembedding(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3c36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, device, train_loader, accum_steps, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for idx, inputs in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = inputs[:,1:]\n",
    "        outputs = model(inputs)[:,:-1,:]\n",
    "\n",
    "        targets = targets.reshape(-1)\n",
    "        outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "        \n",
    "        loss = criterion(outputs, targets) / accum_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (idx + 1) % accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (idx + 1) % (accum_steps * 4) == 0:\n",
    "            print(f\"Epoch [{epoch}].[{idx}] Loss: {loss * accum_steps}\")\n",
    "\n",
    "        if (idx + 1) % (accum_steps * 16) == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"TIME: {elapsed_time / (accum_steps * 16)} seconds per batch\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            allocated = torch.cuda.memory_allocated() / 1e9\n",
    "            reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "            print(f\"USAGE: Allocated {allocated:.2f}GB, Reserved {reserved:.2f}GB, Peak: {peak:.2f}GB\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81cedc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c0e410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2141709\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train+validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7d5ead-bd78-46a6-82cd-cdcec26fe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38761c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequence):\n",
    "    sequence[\"text\"] = torch.tensor(encoder.encode(sequence[\"text\"]), dtype=torch.int64)\n",
    "    return sequence\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, num_proc=8).with_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c17f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset[\"train\"][\"text\"]\n",
    "test = tokenized_dataset[\"test\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf5859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 2\n",
    "accum_steps = 32\n",
    "d_model  = 128\n",
    "d_query  = 64\n",
    "d_up = 256\n",
    "n_heads  = 4\n",
    "n_layers = 4\n",
    "\n",
    "n_vocab  = encoder.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7887f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100277\n"
     ]
    }
   ],
   "source": [
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29143c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padding(batch):\n",
    "    batch = pad_sequence(batch, batch_first=True)\n",
    "    return batch\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "417bc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(n_vocab=n_vocab, \n",
    "                    d_model=d_model, \n",
    "                    d_query=d_query, \n",
    "                    n_heads=n_heads, \n",
    "                    n_layers=n_layers, \n",
    "                    d_up=d_up, \n",
    "                    device=device).to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0117 15:23:36.360000 23141 torch/_inductor/utils.py:1613] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1].[127] Loss: 11.453177452087402\n",
      "Epoch [1].[255] Loss: 11.430920600891113\n",
      "Epoch [1].[383] Loss: 11.328810691833496\n",
      "Epoch [1].[511] Loss: 11.207075119018555\n",
      "TIME: 0.029604391660541296 seconds per batch\n",
      "USAGE: Allocated 0.66GB, Reserved 5.70GB, Peak: 3.16GB\n",
      "Epoch [1].[639] Loss: 10.81448745727539\n",
      "Epoch [1].[767] Loss: 10.579827308654785\n",
      "Epoch [1].[895] Loss: 9.678289413452148\n",
      "Epoch [1].[1023] Loss: 9.82111644744873\n",
      "TIME: 0.01738420268520713 seconds per batch\n",
      "USAGE: Allocated 0.58GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[1151] Loss: 10.141927719116211\n",
      "Epoch [1].[1279] Loss: 9.990823745727539\n",
      "Epoch [1].[1407] Loss: 8.174714088439941\n",
      "Epoch [1].[1535] Loss: 7.564515113830566\n",
      "TIME: 0.017746948171406984 seconds per batch\n",
      "USAGE: Allocated 0.56GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[1663] Loss: 8.040775299072266\n",
      "Epoch [1].[1791] Loss: 8.503752708435059\n",
      "Epoch [1].[1919] Loss: 7.177179336547852\n",
      "Epoch [1].[2047] Loss: 6.9915571212768555\n",
      "TIME: 0.017334952484816313 seconds per batch\n",
      "USAGE: Allocated 0.50GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[2175] Loss: 6.745466709136963\n",
      "Epoch [1].[2303] Loss: 6.518181800842285\n",
      "Epoch [1].[2431] Loss: 5.691226005554199\n",
      "Epoch [1].[2559] Loss: 5.408203125\n",
      "TIME: 0.017279816791415215 seconds per batch\n",
      "USAGE: Allocated 0.69GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[2687] Loss: 5.201603412628174\n",
      "Epoch [1].[2815] Loss: 6.554760932922363\n",
      "Epoch [1].[2943] Loss: 6.211894512176514\n",
      "Epoch [1].[3071] Loss: 5.804749011993408\n",
      "TIME: 0.01757637830451131 seconds per batch\n",
      "USAGE: Allocated 0.60GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[3199] Loss: 6.425682544708252\n",
      "Epoch [1].[3327] Loss: 6.334351062774658\n",
      "Epoch [1].[3455] Loss: 6.188235759735107\n",
      "Epoch [1].[3583] Loss: 6.159965991973877\n",
      "TIME: 0.017061470076441765 seconds per batch\n",
      "USAGE: Allocated 0.53GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[3711] Loss: 5.266104221343994\n",
      "Epoch [1].[3839] Loss: 5.987697601318359\n",
      "Epoch [1].[3967] Loss: 5.418861389160156\n",
      "Epoch [1].[4095] Loss: 5.1292338371276855\n",
      "TIME: 0.017275366932153702 seconds per batch\n",
      "USAGE: Allocated 0.63GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[4223] Loss: 6.001221179962158\n",
      "Epoch [1].[4351] Loss: 6.688164234161377\n",
      "Epoch [1].[4479] Loss: 6.600600719451904\n",
      "Epoch [1].[4607] Loss: 5.484152793884277\n",
      "TIME: 0.016931655816733837 seconds per batch\n",
      "USAGE: Allocated 0.51GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[4735] Loss: 5.799156665802002\n",
      "Epoch [1].[4863] Loss: 5.732996463775635\n",
      "Epoch [1].[4991] Loss: 6.044022560119629\n",
      "Epoch [1].[5119] Loss: 4.777050971984863\n",
      "TIME: 0.0173053415492177 seconds per batch\n",
      "USAGE: Allocated 0.65GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[5247] Loss: 5.193798542022705\n",
      "Epoch [1].[5375] Loss: 5.781639575958252\n",
      "Epoch [1].[5503] Loss: 5.72893762588501\n",
      "Epoch [1].[5631] Loss: 5.825707912445068\n",
      "TIME: 0.017774070147424936 seconds per batch\n",
      "USAGE: Allocated 0.54GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[5759] Loss: 5.421182632446289\n",
      "Epoch [1].[5887] Loss: 5.58051061630249\n",
      "Epoch [1].[6015] Loss: 6.088669776916504\n",
      "Epoch [1].[6143] Loss: 5.824562072753906\n",
      "TIME: 0.016550561413168907 seconds per batch\n",
      "USAGE: Allocated 0.53GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[6271] Loss: 4.813962459564209\n",
      "Epoch [1].[6399] Loss: 5.113304615020752\n",
      "Epoch [1].[6527] Loss: 6.182410717010498\n",
      "Epoch [1].[6655] Loss: 6.302648067474365\n",
      "TIME: 0.017790748737752438 seconds per batch\n",
      "USAGE: Allocated 0.50GB, Reserved 3.98GB, Peak: 3.72GB\n",
      "Epoch [1].[6783] Loss: 5.732583045959473\n",
      "Epoch [1].[6911] Loss: 6.00750207901001\n",
      "Epoch [1].[7039] Loss: 6.0162553787231445\n",
      "Epoch [1].[7167] Loss: 5.442082405090332\n",
      "TIME: 0.01786326989531517 seconds per batch\n",
      "USAGE: Allocated 0.57GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[7295] Loss: 5.133328437805176\n",
      "Epoch [1].[7423] Loss: 5.180650234222412\n",
      "Epoch [1].[7551] Loss: 5.5733137130737305\n",
      "Epoch [1].[7679] Loss: 5.204796314239502\n",
      "TIME: 0.017341080587357283 seconds per batch\n",
      "USAGE: Allocated 0.49GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[7807] Loss: 4.712868690490723\n",
      "Epoch [1].[7935] Loss: 4.964750289916992\n",
      "Epoch [1].[8063] Loss: 6.205904006958008\n",
      "Epoch [1].[8191] Loss: 6.476337909698486\n",
      "TIME: 0.017690219916403294 seconds per batch\n",
      "USAGE: Allocated 0.56GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[8319] Loss: 4.9952921867370605\n",
      "Epoch [1].[8447] Loss: 5.171936988830566\n",
      "Epoch [1].[8575] Loss: 5.504104137420654\n",
      "Epoch [1].[8703] Loss: 5.853038787841797\n",
      "TIME: 0.017176161985844374 seconds per batch\n",
      "USAGE: Allocated 0.48GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[8831] Loss: 4.887521266937256\n",
      "Epoch [1].[8959] Loss: 6.028218746185303\n",
      "Epoch [1].[9087] Loss: 5.681263446807861\n",
      "Epoch [1].[9215] Loss: 5.630908012390137\n",
      "TIME: 0.017670595552772284 seconds per batch\n",
      "USAGE: Allocated 0.53GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[9343] Loss: 5.415446758270264\n",
      "Epoch [1].[9471] Loss: 6.095326900482178\n",
      "Epoch [1].[9599] Loss: 5.6862640380859375\n",
      "Epoch [1].[9727] Loss: 5.683955669403076\n",
      "TIME: 0.01714101294055581 seconds per batch\n",
      "USAGE: Allocated 0.53GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[9855] Loss: 4.795276165008545\n",
      "Epoch [1].[9983] Loss: 6.10789680480957\n",
      "Epoch [1].[10111] Loss: 5.940492630004883\n",
      "Epoch [1].[10239] Loss: 6.4200239181518555\n",
      "TIME: 0.018100992776453495 seconds per batch\n",
      "USAGE: Allocated 0.52GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[10367] Loss: 6.199244022369385\n",
      "Epoch [1].[10495] Loss: 5.48518705368042\n",
      "Epoch [1].[10623] Loss: 4.36686372756958\n",
      "Epoch [1].[10751] Loss: 6.2156081199646\n",
      "TIME: 0.01651565544307232 seconds per batch\n",
      "USAGE: Allocated 0.50GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[10879] Loss: 6.039470672607422\n",
      "Epoch [1].[11007] Loss: 4.801011085510254\n",
      "Epoch [1].[11135] Loss: 5.422272682189941\n",
      "Epoch [1].[11263] Loss: 5.845804691314697\n",
      "TIME: 0.017479665111750364 seconds per batch\n",
      "USAGE: Allocated 0.50GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[11391] Loss: 4.955034255981445\n",
      "Epoch [1].[11519] Loss: 5.784892559051514\n",
      "Epoch [1].[11647] Loss: 5.219976902008057\n",
      "Epoch [1].[11775] Loss: 5.034968376159668\n",
      "TIME: 0.0174077651463449 seconds per batch\n",
      "USAGE: Allocated 0.55GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[11903] Loss: 5.881904602050781\n",
      "Epoch [1].[12031] Loss: 4.315051555633545\n",
      "Epoch [1].[12159] Loss: 4.2063140869140625\n",
      "Epoch [1].[12287] Loss: 5.283465385437012\n",
      "TIME: 0.017043334431946278 seconds per batch\n",
      "USAGE: Allocated 0.50GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[12415] Loss: 5.5664143562316895\n",
      "Epoch [1].[12543] Loss: 5.321134567260742\n",
      "Epoch [1].[12671] Loss: 6.227513790130615\n",
      "Epoch [1].[12799] Loss: 5.140894412994385\n",
      "TIME: 0.01772230677306652 seconds per batch\n",
      "USAGE: Allocated 0.69GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[12927] Loss: 5.005221366882324\n",
      "Epoch [1].[13055] Loss: 4.715475559234619\n",
      "Epoch [1].[13183] Loss: 5.617649555206299\n",
      "Epoch [1].[13311] Loss: 4.244492530822754\n",
      "TIME: 0.017981652170419693 seconds per batch\n",
      "USAGE: Allocated 0.69GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[13439] Loss: 5.411105155944824\n",
      "Epoch [1].[13567] Loss: 5.663370132446289\n",
      "Epoch [1].[13695] Loss: 5.389319896697998\n",
      "Epoch [1].[13823] Loss: 5.527853965759277\n",
      "TIME: 0.01700565032660961 seconds per batch\n",
      "USAGE: Allocated 0.49GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[13951] Loss: 4.683755874633789\n",
      "Epoch [1].[14079] Loss: 5.072065353393555\n",
      "Epoch [1].[14207] Loss: 6.131962776184082\n",
      "Epoch [1].[14335] Loss: 5.7418928146362305\n",
      "TIME: 0.017567246221005917 seconds per batch\n",
      "USAGE: Allocated 0.50GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[14463] Loss: 5.612477779388428\n",
      "Epoch [1].[14591] Loss: 6.179069519042969\n",
      "Epoch [1].[14719] Loss: 4.8694353103637695\n",
      "Epoch [1].[14847] Loss: 5.207573413848877\n",
      "TIME: 0.017263234592974186 seconds per batch\n",
      "USAGE: Allocated 0.52GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[14975] Loss: 4.372722148895264\n",
      "Epoch [1].[15103] Loss: 5.483044147491455\n",
      "Epoch [1].[15231] Loss: 5.600430488586426\n",
      "Epoch [1].[15359] Loss: 5.5713958740234375\n",
      "TIME: 0.01742064906284213 seconds per batch\n",
      "USAGE: Allocated 0.51GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[15487] Loss: 5.482247352600098\n",
      "Epoch [1].[15615] Loss: 5.762624263763428\n",
      "Epoch [1].[15743] Loss: 5.257232666015625\n",
      "Epoch [1].[15871] Loss: 4.073095798492432\n",
      "TIME: 0.017818311229348183 seconds per batch\n",
      "USAGE: Allocated 0.76GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[15999] Loss: 4.5593061447143555\n",
      "Epoch [1].[16127] Loss: 5.247908115386963\n",
      "Epoch [1].[16255] Loss: 5.429911136627197\n",
      "Epoch [1].[16383] Loss: 3.7270286083221436\n",
      "TIME: 0.017254154197871685 seconds per batch\n",
      "USAGE: Allocated 0.74GB, Reserved 4.78GB, Peak: 3.72GB\n",
      "Epoch [1].[16511] Loss: 5.338244438171387\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, criterion, device, train_loader, accum_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(train[:8])\n",
    "# print(f\"output: {output.shape}\")\n",
    "# print(output[0,-4,:].argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
